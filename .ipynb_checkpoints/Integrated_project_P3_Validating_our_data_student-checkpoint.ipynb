{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c121c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6a0d5b",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e22624",
   "metadata": {},
   "source": [
    "## Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a0d4832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(sqlite:///Maji_Ndogo_farm_survey_small.db)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_db_engine('sqlite:///Maji_Ndogo_farm_survey_small.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b0125d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Min_temperature_C</th>\n",
       "      <th>Max_temperature_C</th>\n",
       "      <th>Ave_temps</th>\n",
       "      <th>Soil_fertility</th>\n",
       "      <th>Soil_type</th>\n",
       "      <th>pH</th>\n",
       "      <th>Pollution_level</th>\n",
       "      <th>Plot_size</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>Annual_yield</th>\n",
       "      <th>Standard_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40734</td>\n",
       "      <td>786.05580</td>\n",
       "      <td>-7.389911</td>\n",
       "      <td>-7.556202</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>14.795113</td>\n",
       "      <td>1125.2</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>33.1</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>6.169393</td>\n",
       "      <td>8.526684e-02</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.751354</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.577964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30629</td>\n",
       "      <td>674.33410</td>\n",
       "      <td>-7.736849</td>\n",
       "      <td>-1.051539</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.374611</td>\n",
       "      <td>1450.7</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>30.6</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.676648</td>\n",
       "      <td>3.996838e-01</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.069865</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.486302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39924</td>\n",
       "      <td>826.53390</td>\n",
       "      <td>-9.926616</td>\n",
       "      <td>0.115156</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.339692</td>\n",
       "      <td>2208.9</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>28.4</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.69</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.331993</td>\n",
       "      <td>3.580286e-01</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.208801</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.649647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5754</td>\n",
       "      <td>574.94617</td>\n",
       "      <td>-2.420131</td>\n",
       "      <td>-6.592215</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>7.109855</td>\n",
       "      <td>328.8</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>32.2</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Loamy</td>\n",
       "      <td>5.328150</td>\n",
       "      <td>2.866871e-01</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.277635</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.532348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14146</td>\n",
       "      <td>886.35300</td>\n",
       "      <td>-3.055434</td>\n",
       "      <td>-7.952609</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>55.007656</td>\n",
       "      <td>785.2</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.25</td>\n",
       "      <td>0.72</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.721234</td>\n",
       "      <td>4.319027e-02</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.832614</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.555076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>11472</td>\n",
       "      <td>681.36145</td>\n",
       "      <td>-7.358371</td>\n",
       "      <td>-6.254369</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>16.213196</td>\n",
       "      <td>885.7</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.55</td>\n",
       "      <td>0.61</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.741063</td>\n",
       "      <td>3.286828e-01</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.609930</td>\n",
       "      <td>potato</td>\n",
       "      <td>0.554482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>19660</td>\n",
       "      <td>667.02120</td>\n",
       "      <td>-3.154559</td>\n",
       "      <td>-4.475046</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>2.397553</td>\n",
       "      <td>501.1</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>32.1</td>\n",
       "      <td>13.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.445833</td>\n",
       "      <td>1.602583e-01</td>\n",
       "      <td>8.7</td>\n",
       "      <td>3.812289</td>\n",
       "      <td>maize</td>\n",
       "      <td>0.438194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>41296</td>\n",
       "      <td>670.77900</td>\n",
       "      <td>-14.472861</td>\n",
       "      <td>-6.110221</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>7.636470</td>\n",
       "      <td>1586.6</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.385873</td>\n",
       "      <td>8.221326e-09</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.681629</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.800776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>33090</td>\n",
       "      <td>429.48840</td>\n",
       "      <td>-14.653089</td>\n",
       "      <td>-6.984116</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>13.944720</td>\n",
       "      <td>1272.2</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>34.6</td>\n",
       "      <td>14.20</td>\n",
       "      <td>0.63</td>\n",
       "      <td>Silt</td>\n",
       "      <td>5.562508</td>\n",
       "      <td>6.917245e-10</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.659874</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.507595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>8375</td>\n",
       "      <td>763.09030</td>\n",
       "      <td>-4.317028</td>\n",
       "      <td>-6.344461</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>35.189430</td>\n",
       "      <td>516.4</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>29.6</td>\n",
       "      <td>12.90</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.087792</td>\n",
       "      <td>2.612715e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.226532</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.453064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5654 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Field_ID  Elevation   Latitude  Longitude        Location      Slope  \\\n",
       "0        40734  786.05580  -7.389911  -7.556202    Rural_Akatsi  14.795113   \n",
       "1        30629  674.33410  -7.736849  -1.051539    Rural_Sokoto  11.374611   \n",
       "2        39924  826.53390  -9.926616   0.115156    Rural_Sokoto  11.339692   \n",
       "3         5754  574.94617  -2.420131  -6.592215  Rural_Kilimani   7.109855   \n",
       "4        14146  886.35300  -3.055434  -7.952609  Rural_Kilimani  55.007656   \n",
       "...        ...        ...        ...        ...             ...        ...   \n",
       "5649     11472  681.36145  -7.358371  -6.254369    Rural_Akatsi  16.213196   \n",
       "5650     19660  667.02120  -3.154559  -4.475046  Rural_Kilimani   2.397553   \n",
       "5651     41296  670.77900 -14.472861  -6.110221   Rural_Hawassa   7.636470   \n",
       "5652     33090  429.48840 -14.653089  -6.984116   Rural_Hawassa  13.944720   \n",
       "5653      8375  763.09030  -4.317028  -6.344461  Rural_Kilimani  35.189430   \n",
       "\n",
       "      Rainfall  Min_temperature_C  Max_temperature_C  Ave_temps  \\\n",
       "0       1125.2               -3.1               33.1      15.00   \n",
       "1       1450.7               -3.9               30.6      13.35   \n",
       "2       2208.9               -1.8               28.4      13.30   \n",
       "3        328.8               -5.8               32.2      13.20   \n",
       "4        785.2               -2.5               31.0      14.25   \n",
       "...        ...                ...                ...        ...   \n",
       "5649     885.7               -4.3               33.4      14.55   \n",
       "5650     501.1               -4.8               32.1      13.65   \n",
       "5651    1586.6               -3.8               33.4      14.80   \n",
       "5652    1272.2               -6.2               34.6      14.20   \n",
       "5653     516.4               -3.8               29.6      12.90   \n",
       "\n",
       "      Soil_fertility Soil_type        pH  Pollution_level  Plot_size  \\\n",
       "0               0.62     Sandy  6.169393     8.526684e-02        1.3   \n",
       "1               0.64  Volcanic  5.676648     3.996838e-01        2.2   \n",
       "2               0.69  Volcanic  5.331993     3.580286e-01        3.4   \n",
       "3               0.54     Loamy  5.328150     2.866871e-01        2.4   \n",
       "4               0.72     Sandy  5.721234     4.319027e-02        1.5   \n",
       "...              ...       ...       ...              ...        ...   \n",
       "5649            0.61     Sandy  5.741063     3.286828e-01        1.1   \n",
       "5650            0.54     Sandy  5.445833     1.602583e-01        8.7   \n",
       "5651            0.64  Volcanic  5.385873     8.221326e-09        2.1   \n",
       "5652            0.63      Silt  5.562508     6.917245e-10        1.3   \n",
       "5653            0.64     Sandy  5.087792     2.612715e-01        0.5   \n",
       "\n",
       "      Crop_type Annual_yield  Standard_yield  \n",
       "0      0.751354      cassava        0.577964  \n",
       "1      1.069865      cassava        0.486302  \n",
       "2      2.208801          tea        0.649647  \n",
       "3      1.277635      cassava        0.532348  \n",
       "4      0.832614        wheat        0.555076  \n",
       "...         ...          ...             ...  \n",
       "5649   0.609930       potato        0.554482  \n",
       "5650   3.812289        maize        0.438194  \n",
       "5651   1.681629          tea        0.800776  \n",
       "5652   0.659874      cassava        0.507595  \n",
       "5653   0.226532        wheat        0.453064  \n",
       "\n",
       "[5654 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQL_engine = create_db_engine('sqlite:///Maji_Ndogo_farm_survey_small.db')\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = query_data(SQL_engine, sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b65d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Min_temperature_C</th>\n",
       "      <th>Max_temperature_C</th>\n",
       "      <th>Ave_temps</th>\n",
       "      <th>Soil_fertility</th>\n",
       "      <th>Soil_type</th>\n",
       "      <th>pH</th>\n",
       "      <th>Pollution_level</th>\n",
       "      <th>Plot_size</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>Annual_yield</th>\n",
       "      <th>Standard_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40734</td>\n",
       "      <td>786.05580</td>\n",
       "      <td>-7.389911</td>\n",
       "      <td>-7.556202</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>14.795113</td>\n",
       "      <td>1125.2</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>33.1</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>6.169393</td>\n",
       "      <td>8.526684e-02</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.751354</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.577964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30629</td>\n",
       "      <td>674.33410</td>\n",
       "      <td>-7.736849</td>\n",
       "      <td>-1.051539</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.374611</td>\n",
       "      <td>1450.7</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>30.6</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.676648</td>\n",
       "      <td>3.996838e-01</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.069865</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.486302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39924</td>\n",
       "      <td>826.53390</td>\n",
       "      <td>-9.926616</td>\n",
       "      <td>0.115156</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.339692</td>\n",
       "      <td>2208.9</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>28.4</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.69</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.331993</td>\n",
       "      <td>3.580286e-01</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.208801</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.649647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5754</td>\n",
       "      <td>574.94617</td>\n",
       "      <td>-2.420131</td>\n",
       "      <td>-6.592215</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>7.109855</td>\n",
       "      <td>328.8</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>32.2</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Loamy</td>\n",
       "      <td>5.328150</td>\n",
       "      <td>2.866871e-01</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.277635</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.532348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14146</td>\n",
       "      <td>886.35300</td>\n",
       "      <td>-3.055434</td>\n",
       "      <td>-7.952609</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>55.007656</td>\n",
       "      <td>785.2</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.25</td>\n",
       "      <td>0.72</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.721234</td>\n",
       "      <td>4.319027e-02</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.832614</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.555076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>11472</td>\n",
       "      <td>681.36145</td>\n",
       "      <td>-7.358371</td>\n",
       "      <td>-6.254369</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>16.213196</td>\n",
       "      <td>885.7</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.55</td>\n",
       "      <td>0.61</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.741063</td>\n",
       "      <td>3.286828e-01</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.609930</td>\n",
       "      <td>potato</td>\n",
       "      <td>0.554482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>19660</td>\n",
       "      <td>667.02120</td>\n",
       "      <td>-3.154559</td>\n",
       "      <td>-4.475046</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>2.397553</td>\n",
       "      <td>501.1</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>32.1</td>\n",
       "      <td>13.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.445833</td>\n",
       "      <td>1.602583e-01</td>\n",
       "      <td>8.7</td>\n",
       "      <td>3.812289</td>\n",
       "      <td>maize</td>\n",
       "      <td>0.438194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>41296</td>\n",
       "      <td>670.77900</td>\n",
       "      <td>-14.472861</td>\n",
       "      <td>-6.110221</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>7.636470</td>\n",
       "      <td>1586.6</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.385873</td>\n",
       "      <td>8.221326e-09</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.681629</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.800776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>33090</td>\n",
       "      <td>429.48840</td>\n",
       "      <td>-14.653089</td>\n",
       "      <td>-6.984116</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>13.944720</td>\n",
       "      <td>1272.2</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>34.6</td>\n",
       "      <td>14.20</td>\n",
       "      <td>0.63</td>\n",
       "      <td>Silt</td>\n",
       "      <td>5.562508</td>\n",
       "      <td>6.917245e-10</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.659874</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.507595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>8375</td>\n",
       "      <td>763.09030</td>\n",
       "      <td>-4.317028</td>\n",
       "      <td>-6.344461</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>35.189430</td>\n",
       "      <td>516.4</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>29.6</td>\n",
       "      <td>12.90</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.087792</td>\n",
       "      <td>2.612715e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.226532</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.453064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5654 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Field_ID  Elevation   Latitude  Longitude        Location      Slope  \\\n",
       "0        40734  786.05580  -7.389911  -7.556202    Rural_Akatsi  14.795113   \n",
       "1        30629  674.33410  -7.736849  -1.051539    Rural_Sokoto  11.374611   \n",
       "2        39924  826.53390  -9.926616   0.115156    Rural_Sokoto  11.339692   \n",
       "3         5754  574.94617  -2.420131  -6.592215  Rural_Kilimani   7.109855   \n",
       "4        14146  886.35300  -3.055434  -7.952609  Rural_Kilimani  55.007656   \n",
       "...        ...        ...        ...        ...             ...        ...   \n",
       "5649     11472  681.36145  -7.358371  -6.254369    Rural_Akatsi  16.213196   \n",
       "5650     19660  667.02120  -3.154559  -4.475046  Rural_Kilimani   2.397553   \n",
       "5651     41296  670.77900 -14.472861  -6.110221   Rural_Hawassa   7.636470   \n",
       "5652     33090  429.48840 -14.653089  -6.984116   Rural_Hawassa  13.944720   \n",
       "5653      8375  763.09030  -4.317028  -6.344461  Rural_Kilimani  35.189430   \n",
       "\n",
       "      Rainfall  Min_temperature_C  Max_temperature_C  Ave_temps  \\\n",
       "0       1125.2               -3.1               33.1      15.00   \n",
       "1       1450.7               -3.9               30.6      13.35   \n",
       "2       2208.9               -1.8               28.4      13.30   \n",
       "3        328.8               -5.8               32.2      13.20   \n",
       "4        785.2               -2.5               31.0      14.25   \n",
       "...        ...                ...                ...        ...   \n",
       "5649     885.7               -4.3               33.4      14.55   \n",
       "5650     501.1               -4.8               32.1      13.65   \n",
       "5651    1586.6               -3.8               33.4      14.80   \n",
       "5652    1272.2               -6.2               34.6      14.20   \n",
       "5653     516.4               -3.8               29.6      12.90   \n",
       "\n",
       "      Soil_fertility Soil_type        pH  Pollution_level  Plot_size  \\\n",
       "0               0.62     Sandy  6.169393     8.526684e-02        1.3   \n",
       "1               0.64  Volcanic  5.676648     3.996838e-01        2.2   \n",
       "2               0.69  Volcanic  5.331993     3.580286e-01        3.4   \n",
       "3               0.54     Loamy  5.328150     2.866871e-01        2.4   \n",
       "4               0.72     Sandy  5.721234     4.319027e-02        1.5   \n",
       "...              ...       ...       ...              ...        ...   \n",
       "5649            0.61     Sandy  5.741063     3.286828e-01        1.1   \n",
       "5650            0.54     Sandy  5.445833     1.602583e-01        8.7   \n",
       "5651            0.64  Volcanic  5.385873     8.221326e-09        2.1   \n",
       "5652            0.63      Silt  5.562508     6.917245e-10        1.3   \n",
       "5653            0.64     Sandy  5.087792     2.612715e-01        0.5   \n",
       "\n",
       "      Crop_type Annual_yield  Standard_yield  \n",
       "0      0.751354      cassava        0.577964  \n",
       "1      1.069865      cassava        0.486302  \n",
       "2      2.208801          tea        0.649647  \n",
       "3      1.277635      cassava        0.532348  \n",
       "4      0.832614        wheat        0.555076  \n",
       "...         ...          ...             ...  \n",
       "5649   0.609930       potato        0.554482  \n",
       "5650   3.812289        maize        0.438194  \n",
       "5651   1.681629          tea        0.800776  \n",
       "5652   0.659874      cassava        0.507595  \n",
       "5653   0.226532        wheat        0.453064  \n",
       "\n",
       "[5654 rows x 18 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = query_data(create_db_engine('sqlite:///Maji_Ndogo_farm_survey_small.db'), sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12778c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Name our logger so we know that logs from this module come from the data_ingestion module\n",
    "logger = logging.getLogger('data_ingestion')\n",
    "\n",
    "# Set a basic logging message up that prints out a timestamp, the name of our logger, and the message\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def create_db_engine(db_path):\n",
    "    try:\n",
    "        engine = create_engine(db_path)\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            pass\n",
    "        # test if the database engine was created successfully\n",
    "        logger.info(\"Database engine created successfully.\")\n",
    "        return engine # Return the engine object if it all works well\n",
    "    except ImportError: #If we get an ImportError, inform the user SQLAlchemy is not installed\n",
    "        logger.error(\"SQLAlchemy is required to use this function. Please install it first.\")\n",
    "        raise e\n",
    "    except Exception as e:# If we fail to create an engine inform the user\n",
    "        logger.error(f\"Failed to create database engine. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def query_data(engine, sql_query):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            df = pd.read_sql_query(text(sql_query), connection)\n",
    "        if df.empty:\n",
    "            # Log a message or handle the empty DataFrame scenario as needed\n",
    "            msg = \"The query returned an empty DataFrame.\"\n",
    "            logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "        logger.info(\"Query executed successfully.\")\n",
    "        return df\n",
    "    except ValueError as e: \n",
    "        logger.error(f\"SQL query failed. Error: {e}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while querying the database. Error: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc433f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 18:31:42,671 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 18:31:42,684 - data_ingestion - ERROR - The query returned an empty DataFrame.\n",
      "2024-02-26 18:31:42,684 - data_ingestion - ERROR - SQL query failed. Error: The query returned an empty DataFrame.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The query returned an empty DataFrame.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m      3\u001b[0m sql_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124mSELECT *\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mFROM geographic_features\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124mWHERE Rainfall < 0 \u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# The last line won't ever be true, so no results will be returned. \u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m query_data(SQL_engine, sql_query)\n\u001b[0;32m     14\u001b[0m df\n",
      "Cell \u001b[1;32mIn[10], line 40\u001b[0m, in \u001b[0;36mquery_data\u001b[1;34m(engine, sql_query)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[0;32m     39\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQL query failed. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     42\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while querying the database. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 35\u001b[0m, in \u001b[0;36mquery_data\u001b[1;34m(engine, sql_query)\u001b[0m\n\u001b[0;32m     33\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe query returned an empty DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(msg)\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m     36\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery executed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[1;31mValueError\u001b[0m: The query returned an empty DataFrame."
     ]
    }
   ],
   "source": [
    "SQL_engine = create_db_engine('sqlite:///Maji_Ndogo_farm_survey_small.db')\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "WHERE Rainfall < 0 \n",
    "\"\"\"\n",
    "# The last line won't ever be true, so no results will be returned. \n",
    "\n",
    "df = query_data(SQL_engine, sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31b5eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\")\n",
    "weather_station_mapping_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c8a6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 18:32:52,545 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 18:32:53,460 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    }
   ],
   "source": [
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "\n",
    "def read_from_web_CSV(URL):\n",
    "    try:\n",
    "        df = pd.read_csv(URL)\n",
    "        logger.info(\"CSV file read successfully from the web.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logger.error(\"The URL does not point to a valid CSV file. Please check the URL and try again.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read CSV from the web. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_data = read_from_web_CSV(weather_mapping_data_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0462cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Name our logger so we know that logs from this module come from the data_ingestion module\n",
    "logger = logging.getLogger('data_ingestion')\n",
    "# Set a basic logging message up that prints out a timestamp, the name of our logger, and the message\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "def create_db_engine(db_path):\n",
    "    try:\n",
    "        engine = create_engine(db_path)\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            pass\n",
    "        # test if the database engine was created successfully\n",
    "        logger.info(\"Database engine created successfully.\")\n",
    "        return engine # Return the engine object if it all works well\n",
    "    except ImportError: #If we get an ImportError, inform the user SQLAlchemy is not installed\n",
    "        logger.error(\"SQLAlchemy is required to use this function. Please install it first.\")\n",
    "        raise e\n",
    "    except Exception as e:# If we fail to create an engine inform the user\n",
    "        logger.error(f\"Failed to create database engine. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def query_data(engine, sql_query):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            df = pd.read_sql_query(text(sql_query), connection)\n",
    "        if df.empty:\n",
    "            # Log a message or handle the empty DataFrame scenario as needed\n",
    "            msg = \"The query returned an empty DataFrame.\"\n",
    "            logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "        logger.info(\"Query executed successfully.\")\n",
    "        return df\n",
    "    except ValueError as e: \n",
    "        logger.error(f\"SQL query failed. Error: {e}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while querying the database. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def read_from_web_CSV(URL):\n",
    "    try:\n",
    "        df = pd.read_csv(URL)\n",
    "        logger.info(\"CSV file read successfully from the web.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logger.error(\"The URL does not point to a valid CSV file. Please check the URL and try again.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read CSV from the web. Error: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39a85194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 18:33:10,353 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 18:33:10,514 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 18:33:11,432 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 18:33:12,386 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    }
   ],
   "source": [
    "# Testing module functions  \n",
    "field_df = query_data(create_db_engine(db_path), sql_query)   \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_df = read_from_web_CSV(weather_mapping_data_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "977a2cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field_df: (5654, 18), weather_df: (1843, 2), weather_mapping_df: (5654, 3)\n"
     ]
    }
   ],
   "source": [
    "field_test = field_df.shape\n",
    "weather_test = weather_df.shape\n",
    "weather_mapping_test = weather_mapping_df.shape\n",
    "print(f\"field_df: {field_test}, weather_df: {weather_test}, weather_mapping_df: {weather_mapping_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9105b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module: data_ingestion\n",
    "\n",
    "functions to ingest data from various data sorces are defined.\n",
    "\n",
    "\"\"\"\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import pandas as pd\n",
    "# Name our logger so we know that logs from this module come from the data_ingestion module\n",
    "logger = logging.getLogger('data_ingestion')\n",
    "# Set a basic logging message up that prints out a timestamp, the name of our logger, and the message\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8f2e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "\n",
    "def create_db_engine(db_path):\n",
    "    \"\"\"creates a SQLAlchemy engine for a database that can be used to conect to the database.\n",
    "\n",
    "    args:\n",
    "    db_path: the .db file path(str) which is in the same directory as the notebook, and matches the file name.\n",
    "    \n",
    "    It returns the database engine object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(db_path)\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            pass\n",
    "        # test if the database engine was created successfully\n",
    "        logger.info(\"Database engine created successfully.\")\n",
    "        return engine # Return the engine object if it all works well\n",
    "    except ImportError: #If we get an ImportError, inform the user SQLAlchemy is not installed\n",
    "        logger.error(\"SQLAlchemy is required to use this function. Please install it first.\")\n",
    "        raise e\n",
    "    except Exception as e:# If we fail to create an engine inform the user\n",
    "        logger.error(f\"Failed to create database engine. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def query_data(engine, sql_query):\n",
    "    \"\"\"Accesses and executes the SQL query on the SQLAlchemy database engine provided and returns a DataFrame\n",
    "\n",
    "    args:\n",
    "    engine: the SQLAlchemy database engine provided\n",
    "    sql_query: The SQL query to excecute on the engine \n",
    "\n",
    "    It returns a DataFrame, with the SQL query applied\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            df = pd.read_sql_query(text(sql_query), connection)\n",
    "        if df.empty:\n",
    "            # Log a message or handle the empty DataFrame scenario as needed\n",
    "            msg = \"The query returned an empty DataFrame.\"\n",
    "            logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "        logger.info(\"Query executed successfully.\")\n",
    "        return df\n",
    "    except ValueError as e: \n",
    "        logger.error(f\"SQL query failed. Error: {e}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while querying the database. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def read_from_web_CSV(URL):\n",
    "    \"\"\"Reads a CSV file frome the web and returns a DataFrame\n",
    "    Args:\n",
    "    URL: the URL of the CSV file from the web.\n",
    "    It returns a Pandas Dataframe containing data from the CSV file in the URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(URL)\n",
    "        logger.info(\"CSV file read successfully from the web.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logger.error(\"The URL does not point to a valid CSV file. Please check the URL and try again.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read CSV from the web. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27dcc193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 18:42:32,630 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 18:42:32,967 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 18:42:34,085 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 18:42:35,078 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field_df: (5654, 18), weather_df: (1843, 2), weather_mapping_df: (5654, 3)\n"
     ]
    }
   ],
   "source": [
    "# Testing module functions  \n",
    "field_df = query_data(create_db_engine(db_path), sql_query)   \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_df = read_from_web_CSV(weather_mapping_data_URL)\n",
    "\n",
    "field_test = field_df.shape\n",
    "weather_test = weather_df.shape\n",
    "weather_mapping_test = weather_mapping_df.shape\n",
    "print(f\"field_df: {field_test}, weather_df: {weather_test}, weather_mapping_df: {weather_mapping_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab292cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_ingestion\n",
      "data_ingestion\n",
      "data_ingestion\n"
     ]
    }
   ],
   "source": [
    "# Importing our new module\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "\n",
    "#Checking if the function names are now associated with the module\n",
    "print(create_db_engine.__module__)\n",
    "print(query_data.__module__)\n",
    "print(read_from_web_CSV.__module__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87170695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 18:42:43,077 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 18:42:43,236 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 18:42:44,193 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 18:42:45,077 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field_df: (5654, 18), weather_df: (1843, 2), weather_mapping_df: (5654, 3)\n"
     ]
    }
   ],
   "source": [
    "field_df = query_data(create_db_engine(db_path), sql_query)   \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_df = read_from_web_CSV(weather_mapping_data_URL)\n",
    "\n",
    "field_test = field_df.shape\n",
    "weather_test = weather_df.shape\n",
    "weather_mapping_test = weather_mapping_df.shape\n",
    "print(f\"field_df: {field_test}, weather_df: {weather_test}, weather_mapping_df: {weather_mapping_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797db70",
   "metadata": {},
   "source": [
    "## Field data processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d4179f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "    \n",
    "    \"\"\"Processes field data.\n",
    "    Args:\n",
    "        logging_level (str, optional):logging level.\n",
    "        \n",
    "    Methods:\n",
    "        \n",
    "        ingest_sql_data(): Ingests data from a database.\n",
    "        rename_columns(): Renames columns.\n",
    "        process(): Executes the code.\n",
    "        apply_corrections(column_name='Crop_type', abs_column='Elevation'): Applies corrections.\n",
    "        initialize_logging(logging_level): Initializes logging for the instance.\n",
    "        weather_station_mapping(): Maps weather station data to the DataFrame.\n",
    "        \n",
    "\n",
    "    Attributes:\n",
    "        weather_map_data : weather_mapping CSV file.\n",
    "        sql_query (str): The query to be executed.\n",
    "        values_to_rename :  values to be  renamed (dict).\n",
    "        logger (logging.Logger): The logger object to analyze our steps.\n",
    "        engine (object):The database engine object.\n",
    "        columns_to_rename: columns to be renamed (dict).\n",
    "        db_path (str):database path.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    def __init__(self, logging_level=\"INFO\"):# When we instantiate this class, we can optionally specify what logs we want to see\n",
    "        \"\"\" initializes the class instance\n",
    "        logging_level (str, optional): The logging level.\"\"\"\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"\n",
    "            SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "        self.initialize_logging(logging_level)\n",
    "        \n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class. \n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # DataFrame methods \n",
    "    def ingest_sql_data(self):\n",
    "      \"\"\" This method returns the initial dataframe\"\"\"  \n",
    "        # First we want to get the data from the SQL database\n",
    "        pass\n",
    "    \n",
    "    def rename_columns(self):\n",
    "        \"\"\"Renames columns.\"\"\"\n",
    "        # Annual_yield and Crop_type must be swapped\n",
    "        pass\n",
    "\n",
    "    def apply_corrections(self):\n",
    "        \"\"\" appllies corrections to the mispelled words in the dataframe\"\"\"\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "       \"\"\"Maps weather station data the dataframe.\"\"\" \n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Executes the entire pipeline.\"\"\"\n",
    "        # This process calls the correct methods and applies the changes, step by step. This is the method we will call, and it will call the other methods in order\n",
    "        \n",
    "        weather_map_df = self.weather_station_mapping() \n",
    "        self.df = self.ingest_sql_data()\n",
    "        self.df = self.rename_columns()\n",
    "        self.df = self.apply_corrections()\n",
    "        self.df = self.df.merge(weather_map_df, on='Field_ID', how='left')\n",
    "        self.df = self.df.drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f30d2d73",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'merge'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This code won't run for now, since we have not defined all of the methods.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m field_processor \u001b[38;5;241m=\u001b[39m FieldDataProcessor()\n\u001b[1;32m----> 3\u001b[0m field_processor\u001b[38;5;241m.\u001b[39mprocess()\n",
      "Cell \u001b[1;32mIn[22], line 113\u001b[0m, in \u001b[0;36mFieldDataProcessor.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrename_columns()\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_corrections()\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mmerge(weather_map_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mField_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'merge'"
     ]
    }
   ],
   "source": [
    "# This code won't run for now, since we have not defined all of the methods.\n",
    "field_processor = FieldDataProcessor()\n",
    "field_processor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe519e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_df = field_processor.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfabeb1",
   "metadata": {},
   "source": [
    "### `def ingest_sql_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "093fec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "    \"\"\"Processes field data.\n",
    "    Args:\n",
    "        logging_level (str, optional):logging level.\n",
    "        \n",
    "    Methods:\n",
    "        \n",
    "        ingest_sql_data(): Ingests data from a database.\n",
    "        rename_columns(): Renames columns.\n",
    "        process(): Executes the code.\n",
    "        apply_corrections(column_name='Crop_type', abs_column='Elevation'): Applies corrections.\n",
    "        initialize_logging(logging_level): Initializes logging for the instance.\n",
    "        weather_station_mapping(): Maps weather station data to the DataFrame.\n",
    "        \n",
    "\n",
    "    Attributes:\n",
    "        weather_map_data : weather_mapping CSV file.\n",
    "        sql_query (str): The query to be executed.\n",
    "        values_to_rename :  values to be  renamed (dict).\n",
    "        logger (logging.Logger): The logger object to analyze our steps.\n",
    "        engine (object):The database engine object.\n",
    "        columns_to_rename: columns to be renamed (dict).\n",
    "        db_path (str):database path.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "      \n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "        \"\"\" initializes the class instance\n",
    "        logging_level (str, optional): The logging level.\"\"\"\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    \"\"\" This method returns the initial dataframe\n",
    "    \"\"\"\n",
    "    def ingest_sql_data(self):\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        return self.df\n",
    "\n",
    "\n",
    "    def rename_columns(self):\n",
    "        \"\"\"Renames columns.\"\"\"\n",
    "        # Annual_yield and Crop_type must be swapped\n",
    "        pass\n",
    "\n",
    "    def apply_corrections(self):\n",
    "        \"\"\" appllies corrections to the mispelled words in the dataframe\"\"\"\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "       \"\"\"Maps weather station data the dataframe.\"\"\"\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Executes the entire pipeline.\"\"\"\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b3f776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 18:55:29,190 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 18:55:29,349 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 18:55:29,349 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 18)\n"
     ]
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.ingest_sql_data()\n",
    "field_df = field_processor.df\n",
    "print(field_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676579c",
   "metadata": {},
   "source": [
    "### `def rename_columns()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "369c190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy in your class including the ingest_sql_data method here\n",
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "    \"\"\"Processes field data.\n",
    "    Args:\n",
    "        logging_level (str, optional):logging level.\n",
    "        \n",
    "    Methods:\n",
    "        \n",
    "        ingest_sql_data(): Ingests data from a database.\n",
    "        rename_columns(): Renames columns.\n",
    "        process(): Executes the code.\n",
    "        apply_corrections(column_name='Crop_type', abs_column='Elevation'): Applies corrections.\n",
    "        initialize_logging(logging_level): Initializes logging for the instance.\n",
    "        weather_station_mapping(): Maps weather station data to the DataFrame.\n",
    "        \n",
    "\n",
    "    Attributes:\n",
    "        weather_map_data : weather_mapping CSV file.\n",
    "        sql_query (str): The query to be executed.\n",
    "        values_to_rename :  values to be  renamed (dict).\n",
    "        logger (logging.Logger): The logger object to analyze our steps.\n",
    "        engine (object):The database engine object.\n",
    "        columns_to_rename: columns to be renamed (dict).\n",
    "        db_path (str):database path.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "      \n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "        \"\"\" initializes the class instance\n",
    "        logging_level (str, optional): The logging level.\"\"\"\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    \"\"\" This method returns the initial dataframe\n",
    "    \"\"\"\n",
    "    def ingest_sql_data(self):\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        return self.df\n",
    "    \n",
    "    def rename_columns(self):\n",
    "        \"\"\"Renames columns.\"\"\"\n",
    "        # Extract the columns to rename from the configuration\n",
    "        column1, column2 = list(self.columns_to_rename.keys())[0], list(self.columns_to_rename.values())[0]   \n",
    "\n",
    "         # Temporarily rename one of the columns to avoid a naming conflict\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "            \n",
    "       \n",
    "        # Perform the swap \n",
    "        \n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})\n",
    "        \n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "            \n",
    "    def apply_corrections(self):\n",
    "        \"\"\" appllies corrections to the mispelled words in the dataframe\"\"\"\n",
    "        \n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        \"\"\"Maps weather station data the dataframe.\"\"\"\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Executes the entire pipeline.\"\"\"\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7233b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 18:59:16,604 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 18:59:16,774 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 18:59:16,776 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-02-26 18:59:16,790 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.751354\n",
       "1    1.069865\n",
       "2    2.208801\n",
       "Name: Annual_yield, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.ingest_sql_data()\n",
    "field_processor.rename_columns()\n",
    "field_df = field_processor.df\n",
    "field_df['Annual_yield'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e74278",
   "metadata": {},
   "source": [
    "### `def apply_corrections()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3065e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy in your class including the ingest_sql_data and  method here\n",
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "    \"\"\"Processes field data.\n",
    "    Args:\n",
    "        logging_level (str, optional):logging level.\n",
    "        \n",
    "    Methods:\n",
    "        \n",
    "        ingest_sql_data(): Ingests data from a database.\n",
    "        rename_columns(): Renames columns.\n",
    "        process(): Executes the code.\n",
    "        apply_corrections(column_name='Crop_type', abs_column='Elevation'): Applies corrections.\n",
    "        initialize_logging(logging_level): Initializes logging for the instance.\n",
    "        weather_station_mapping(): Maps weather station data to the DataFrame.\n",
    "        \n",
    "\n",
    "    Attributes:\n",
    "        weather_map_data : weather_mapping CSV file.\n",
    "        sql_query (str): The query to be executed.\n",
    "        values_to_rename :  values to be  renamed (dict).\n",
    "        logger (logging.Logger): The logger object to analyze our steps.\n",
    "        engine (object):The database engine object.\n",
    "        columns_to_rename: columns to be renamed (dict).\n",
    "        db_path (str):database path.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "      \n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "        \"\"\" initializes the class instance\n",
    "        logging_level (str, optional): The logging level.\"\"\"\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    \"\"\" This method returns the initial dataframe\n",
    "    \"\"\"\n",
    "    def ingest_sql_data(self):\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        return self.df\n",
    "    \n",
    "    def rename_columns(self):\n",
    "        \"\"\"Renames columns.\"\"\"\n",
    "        # Extract the columns to rename from the configuration\n",
    "        column1, column2 = list(self.columns_to_rename.keys())[0], list(self.columns_to_rename.values())[0]   \n",
    "\n",
    "         # Temporarily rename one of the columns to avoid a naming conflict\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "            \n",
    "       \n",
    "        # Perform the swap \n",
    "        \n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})\n",
    "        \n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "    def apply_corrections(self, column_name='Crop_type', abs_column='Elevation'):\n",
    "        \n",
    "        \"\"\" appllies corrections to the mispelled words in the dataframe\n",
    "        \n",
    "        \"\"\"\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "\n",
    "        self.df[abs_column] = self.df[abs_column].abs()\n",
    "        self.df[column_name] = self.df[column_name].apply(lambda crop: self.values_to_rename.get(crop, crop))\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        \"\"\"Maps weather station data the dataframe.\"\"\"\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Executes the entire pipeline.\"\"\"\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e33a085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 19:01:51,899 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 19:01:52,060 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 19:01:52,061 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-02-26 19:01:52,066 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Min_temperature_C</th>\n",
       "      <th>Max_temperature_C</th>\n",
       "      <th>Ave_temps</th>\n",
       "      <th>Soil_fertility</th>\n",
       "      <th>Soil_type</th>\n",
       "      <th>pH</th>\n",
       "      <th>Pollution_level</th>\n",
       "      <th>Plot_size</th>\n",
       "      <th>Annual_yield</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>Standard_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Field_ID, Elevation, Latitude, Longitude, Location, Slope, Rainfall, Min_temperature_C, Max_temperature_C, Ave_temps, Soil_fertility, Soil_type, pH, Pollution_level, Plot_size, Annual_yield, Crop_type, Standard_yield]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.ingest_sql_data()\n",
    "field_processor.rename_columns()\n",
    "field_processor.apply_corrections()\n",
    "\n",
    "field_df = field_processor.df\n",
    "field_df.query(\"Crop_type in ['cassaval','wheatn']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794ee01",
   "metadata": {},
   "source": [
    "### `def weather_station_mapping()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29c58353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy in your class including the ingest_sql_data and method here\n",
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "    \"\"\"Processes field data.\n",
    "    Args:\n",
    "        logging_level (str, optional):logging level.\n",
    "        \n",
    "    Methods:\n",
    "        \n",
    "        ingest_sql_data(): Ingests data from a database.\n",
    "        rename_columns(): Renames columns.\n",
    "        process(): Executes the code.\n",
    "        apply_corrections(column_name='Crop_type', abs_column='Elevation'): Applies corrections.\n",
    "        initialize_logging(logging_level): Initializes logging for the instance.\n",
    "        weather_station_mapping(): Maps weather station data to the DataFrame.\n",
    "        \n",
    "\n",
    "    Attributes:\n",
    "        weather_map_data : weather_mapping CSV file.\n",
    "        sql_query (str): The query to be executed.\n",
    "        values_to_rename :  values to be  renamed (dict).\n",
    "        logger (logging.Logger): The logger object to analyze our steps.\n",
    "        engine (object):The database engine object.\n",
    "        columns_to_rename: columns to be renamed (dict).\n",
    "        db_path (str):database path.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "      \n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "        \"\"\" initializes the class instance\n",
    "        logging_level (str, optional): The logging level.\"\"\"\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        \"\"\" This method returns the initial dataframe\n",
    "        \"\"\"\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        return self.df\n",
    "    \n",
    "    def rename_columns(self):\n",
    "         \"\"\"Renames columns.\"\"\"\n",
    "        # Extract the columns to rename from the configuration\n",
    "       \n",
    "        column1, column2 = list(self.columns_to_rename.keys())[0], list(self.columns_to_rename.values())[0]   \n",
    "\n",
    "         # Temporarily rename one of the columns to avoid a naming conflict\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "            \n",
    "       \n",
    "        # Perform the swap \n",
    "        \n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})\n",
    "        \n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "    def apply_corrections(self, column_name='Crop_type', abs_column='Elevation'):\n",
    "        \"\"\" appllies corrections to the mispelled words in the dataframe\n",
    "        \n",
    "        \"\"\"\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        \n",
    "\n",
    "        self.df[abs_column] = self.df[abs_column].abs()\n",
    "        self.df[column_name] = self.df[column_name].apply(lambda crop: self.values_to_rename.get(crop, crop))\n",
    "    def weather_station_mapping(self):\n",
    "        \"\"\"Maps weather station data the dataframe.\"\"\"\n",
    "\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "\n",
    "        common_column = \"Field_ID\"\n",
    "        self.weather_station_df = read_from_web_CSV(self.weather_map_data)\n",
    "        self.df = pd.merge(self.df, self.weather_station_df, on = common_column)\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Executes the entire pipeline.\"\"\"\n",
    "        # This process calls the correct methods and applies the changes, step by step. This is the method we will call, and it will call the other methods in order.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c05b721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 19:07:51,028 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 19:07:51,181 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 19:07:51,181 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-02-26 19:07:51,190 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-02-26 19:07:53,737 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.ingest_sql_data()\n",
    "field_processor.rename_columns()\n",
    "field_processor.apply_corrections()\n",
    "field_processor.weather_station_mapping()\n",
    "\n",
    "field_df = field_processor.df\n",
    "field_df['Weather_station'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552da32c",
   "metadata": {},
   "source": [
    "### `def process()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d49d1-37bd-4712-9dcc-5e87ce80977c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e526e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy in your class including the ingest_sql_data and  method here\n",
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "    \"\"\"Processes field data.\n",
    "    Args:\n",
    "        logging_level (str, optional):logging level.\n",
    "        \n",
    "    Methods:\n",
    "        \n",
    "        ingest_sql_data(): Ingests data from a database.\n",
    "        rename_columns(): Renames columns.\n",
    "        process(): Executes the code.\n",
    "        apply_corrections(column_name='Crop_type', abs_column='Elevation'): Applies corrections.\n",
    "        initialize_logging(logging_level): Initializes logging for the instance.\n",
    "        weather_station_mapping(): Maps weather station data to the DataFrame.\n",
    "        \n",
    "\n",
    "    Attributes:\n",
    "        weather_map_data : weather_mapping CSV file.\n",
    "        sql_query (str): The query to be executed.\n",
    "        values_to_rename :  values to be  renamed (dict).\n",
    "        logger (logging.Logger): The logger object to analyze our steps.\n",
    "        engine (object):The database engine object.\n",
    "        columns_to_rename: columns to be renamed (dict).\n",
    "        db_path (str):database path.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "      \n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "        \"\"\" initializes the class instance\n",
    "        logging_level (str, optional): The logging level.\"\"\"\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    \n",
    "    def ingest_sql_data(self):\n",
    "        \"\"\" This method returns the initial dataframe\"\"\"\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        return self.df\n",
    "    \n",
    "    def rename_columns(self):\n",
    "        \"\"\"Renames columns.\"\"\"\n",
    "        # Extract the columns to rename from the configuration\n",
    "        column1, column2 = list(self.columns_to_rename.keys())[0], list(self.columns_to_rename.values())[0]   \n",
    "\n",
    "         # Temporarily rename one of the columns to avoid a naming conflict\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "            \n",
    "       \n",
    "        # Perform the swap \n",
    "        \n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})\n",
    "        \n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "    def apply_corrections(self, column_name='Crop_type', abs_column='Elevation'):\n",
    " \n",
    "        \n",
    "        \"\"\" appllies corrections to the mispelled words in the dataframe\n",
    "        \n",
    "        \"\"\"\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        self.df[abs_column] = self.df[abs_column].abs()\n",
    "        self.df[column_name] = self.df[column_name].apply(lambda crop: self.values_to_rename.get(crop, crop))\n",
    "    def weather_station_mapping(self):\n",
    "        \"\"\"Maps weather station data the dataframe.\"\"\"\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        common_column = \"Field_ID\"\n",
    "        self.weather_station_df = read_from_web_CSV(self.weather_map_data)\n",
    "        self.df = pd.merge(self.df, self.weather_station_df, on = common_column)\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Executes the entire pipeline.\"\"\"\n",
    "        self.ingest_sql_data()\n",
    "        #Insert your code here\n",
    "        self.rename_columns()\n",
    "        self.apply_corrections()\n",
    "        self.weather_station_mapping()\n",
    "        self.df = self.df.drop(columns=\"Unnamed: 0\")\n",
    "        return self.df\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3645f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 19:10:54,226 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 19:10:54,598 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 19:10:54,599 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-02-26 19:10:54,604 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-02-26 19:10:55,856 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.process()\n",
    "\n",
    "field_df = field_processor.df\n",
    "field_df['Weather_station'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70083d45",
   "metadata": {},
   "source": [
    "### Centralising the data pipeline configuration details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b70f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the data_ingestion.py module\n",
    "\n",
    "db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "\n",
    "# From the field_data_processor class\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57ec9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_params = {   \"sql_query\": \"\"\"\n",
    "                SELECT *\n",
    "                FROM geographic_features\n",
    "                LEFT JOIN weather_features USING (Field_ID)\n",
    "                LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "                LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\", # Insert your SQL query\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db', # Insert the db_path of the database\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}, # Insert the disctionary of columns we want to swop the names of,\n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}, # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c0922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove these lines from the data_ingestion.py module\n",
    "db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c62c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "\n",
    "class FieldDataProcessor:\n",
    "    \"\"\"Processes field data.\n",
    "    Args:\n",
    "        logging_level (str, optional):logging level.\n",
    "        \n",
    "    Methods:\n",
    "        \n",
    "        ingest_sql_data(): Ingests data from a database.\n",
    "        rename_columns(): Renames columns.\n",
    "        process(): Executes the code.\n",
    "        apply_corrections(column_name='Crop_type', abs_column='Elevation'): Applies corrections.\n",
    "        initialize_logging(logging_level): Initializes logging for the instance.\n",
    "        weather_station_mapping(): Maps weather station data to the DataFrame.\n",
    "        \n",
    "\n",
    "    Attributes:\n",
    "        weather_map_data : weather_mapping CSV file.\n",
    "        sql_query (str): The query to be executed.\n",
    "        values_to_rename :  values to be  renamed (dict).\n",
    "        logger (logging.Logger): The logger object to analyze our steps.\n",
    "        engine (object):The database engine object.\n",
    "        columns_to_rename: columns to be renamed (dict).\n",
    "        db_path (str):database path.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_params, logging_level=\"INFO\"):\n",
    "        \"\"\" initializes the class instance\n",
    "        logging_level (str, optional): The logging level.\"\"\"\n",
    "        self.db_path = config_params['db_path']\n",
    "        self.sql_query = config_params['sql_query']\n",
    "        self.columns_to_rename = config_params['columns_to_rename']\n",
    "        self.values_to_rename = config_params['values_to_rename']\n",
    "        self.weather_map_data = config_params['weather_mapping_csv']\n",
    "\n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False\n",
    "\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "    def ingest_sql_data(self):\n",
    "        \"\"\" This method returns the initial dataframe\"\"\"\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Successfully loaded data.\")\n",
    "        return self.df\n",
    "\n",
    "    def rename_columns(self):\n",
    "        \"\"\"Renames columns.\"\"\"\n",
    "        column1, column2 = list(self.columns_to_rename.keys())[0], list(self.columns_to_rename.values())[0]\n",
    "\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "\n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "\n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})\n",
    "\n",
    "    def apply_corrections(self, column_name='Crop_type', abs_column='Elevation'):\n",
    "        \"\"\" appllies corrections to the mispelled words in the dataframe\n",
    "        \n",
    "        \"\"\"\n",
    "        self.df[abs_column] = self.df[abs_column].abs()\n",
    "        self.df[column_name] = self.df[column_name].apply(lambda crop: self.values_to_rename.get(crop, crop))\n",
    "        self.df[column_name] = self.df[column_name].str.strip()\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        \"\"\"Maps weather station data the dataframe.\"\"\"\n",
    "        weather_map_df = read_from_web_CSV(self.weather_map_data)\n",
    "        self.df = self.df.merge(weather_map_df, on='Field_ID', how='left')\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Executes the entire pipeline.\"\"\"\n",
    "        self.ingest_sql_data()\n",
    "        self.rename_columns()\n",
    "        self.apply_corrections()\n",
    "        self.weather_station_mapping()\n",
    "        self.df = self.df.drop(columns=\"Unnamed: 0\")\n",
    "        return self.df\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8712fd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 19:22:27,322 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 19:22:27,501 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 19:22:27,501 - __main__.FieldDataProcessor - INFO - Successfully loaded data.\n",
      "2024-02-26 19:22:27,501 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-02-26 19:22:32,736 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_params ={ \"sql_query\": \"\"\"\n",
    "                SELECT *\n",
    "                FROM geographic_features\n",
    "                LEFT JOIN weather_features USING (Field_ID)\n",
    "                LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "                LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\", # Insert your SQL query\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db', # Insert the db_path of the database\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}, # Insert the disctionary of columns we want to swop the names of,\n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}, # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "} # Paste in your config_params dictionary here\n",
    "\n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "\n",
    "field_df = field_processor.df\n",
    "field_df['Weather_station'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcad5629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 19:33:00,832 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 19:33:01,076 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 19:33:01,078 - field_data_processor.FieldDataProcessor - INFO - Successfully loaded data.\n",
      "2024-02-26 19:33:01,081 - field_data_processor.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-02-26 19:33:02,439 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # Importing all the packages we will use eventually\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from field_data_processor import FieldDataProcessor # Importing our new module\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params = {\n",
    "    \"sql_query\": \"\"\"\n",
    "                SELECT *\n",
    "                FROM geographic_features\n",
    "                LEFT JOIN weather_features USING (Field_ID)\n",
    "                LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "                LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\", # Insert your SQL query\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db', # Insert the db_path of the database\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}, # Insert the disctionary of columns we want to swop the names of,\n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}, # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "}# Paste in your config_params dictionary here\n",
    "\n",
    "\n",
    "# Instantiating the class with config_params passed to the class as a parameter \n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "# Test\n",
    "field_df['Weather_station'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04508d",
   "metadata": {},
   "source": [
    "## Weather data processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8af63063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Importing the regex pattern\n",
    "import numpy as np\n",
    "\n",
    "weather_station_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\")\n",
    "weather_station_mapping_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\")\n",
    "\n",
    "patterns = {\n",
    "    'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "     'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "    }\n",
    "\n",
    "def extract_measurement(message):\n",
    "    \"\"\"\n",
    "    Extracts a numeric measurement value from a given message string.\n",
    "\n",
    "    The function applies regular expressions to identify and extract\n",
    "    numeric values related to different types of measurements such as\n",
    "    Rainfall, Average Temperatures, and Pollution Levels from a text message.\n",
    "    It returns the key of the matching record, and first matching value as a floating-point number.\n",
    "    \n",
    "    Parameters:\n",
    "    message (str): A string message containing the measurement information.\n",
    "\n",
    "    Returns:\n",
    "    float: The extracted numeric value of the measurement if a match is found;\n",
    "           otherwise, None.\n",
    "\n",
    "    The function uses the following patterns for extraction:\n",
    "    - Rainfall: Matches numbers (including decimal) followed by 'mm', optionally spaced.\n",
    "    - Ave_temps: Matches numbers (including decimal) followed by 'C', optionally spaced.\n",
    "    - Pollution_level: Matches numbers (including decimal) following 'Pollution at' or '='.\n",
    "    \n",
    "    Example usage:\n",
    "    extract_measurement(\"【2022-01-04 21:47:48】温度感应: 现在温度是 12.82C.\")\n",
    "    # Returns: 'Temperature', 12.82\n",
    "    \"\"\"\n",
    "    \n",
    "    for key, pattern in patterns.items(): # Loop through all of the patterns and check if it matches the pattern value.\n",
    "        match = re.search(pattern, message)\n",
    "        if match:\n",
    "            # Extract the first group that matches, which should be the measurement value if all previous matches are empty.\n",
    "            # print(match.groups()) # Uncomment this line to help you debug your regex patterns.\n",
    "            return key, float(next((x for x in match.groups() if x is not None)))\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# The function creates a tuple with the measurement type and value into a Pandas Series\n",
    "result = weather_station_df['Message'].apply(extract_measurement)\n",
    "\n",
    "# Create separate columns for 'Measurement' and 'extracted_value' by unpacking the tuple with Lambda functions.\n",
    "weather_station_df['Measurement'] = result.apply(lambda x: x[0])\n",
    "weather_station_df['Value'] = result.apply(lambda x: x[1])\n",
    "\n",
    "# The function creates a tuple with the measurement type and value into a Pandas Series\n",
    "result = weather_station_df['Message'].apply(extract_measurement)\n",
    "\n",
    "# Create separate columns for 'Measurement' and 'extracted_value' by unpacking the tuple with Lambda functions.\n",
    "weather_station_df['Measurement'] = result.apply(lambda x: x[0])\n",
    "weather_station_df['Value'] = result.apply(lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a30fc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\")\n",
    "weather_station_mapping_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\")\n",
    "\n",
    "patterns = {\n",
    "    'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "     'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "    }\n",
    "\n",
    "config_params = { \n",
    "    \"sql_query\": \"\"\"\n",
    "                SELECT *\n",
    "                FROM geographic_features\n",
    "                LEFT JOIN weather_features USING (Field_ID)\n",
    "                LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "                LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\", # Insert your SQL query\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db', # Insert the db_path of the database\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}, # Insert the disctionary of columns we want to swop the names of,\n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}, # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "    # Paste in your previous dictionary data in here\n",
    "\n",
    "    # Add two new keys\n",
    "   \"weather_csv_path\":  \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the URL for the weather station data\n",
    "    \"regex_patterns\" : {\n",
    "    'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "     'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "    } , # Insert the regex pattern we used to process the messages\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f56158be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the imports we're going to use in the weather data processing module\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from data_ingestion import read_from_web_CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "230dfb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION \n",
    "\n",
    "class WeatherDataProcessor:\n",
    "    \"\"\"Processes weather data based on configuration parameters.\n",
    "\n",
    "    Args:\n",
    "        config_params (dict): A dictionary containing configuration parameters.\n",
    "        logging_level (str, optional): The logging level. Defaults to \"INFO\".\n",
    "\n",
    "    Attributes:\n",
    "        weather_station_data (str): The path to the weather CSV file.\n",
    "        patterns (dict): A dictionary mapping measurement keys to regex patterns.\n",
    "        weather_df (DataFrame): The DataFrame containing weather data.\n",
    "        logger (logging.Logger): The logger object.\n",
    "\n",
    "    Methods:\n",
    "        initialize_logging(logging_level): Initializes logging for the instance.\n",
    "        weather_station_mapping(): Maps weather station data to the DataFrame.\n",
    "        extract_measurement(message): Extracts a measurement from a message using regex patterns.\n",
    "        process_messages(): Processes messages to extract measurements.\n",
    "        calculate_means(): Calculates the mean values of measurements.\n",
    "        process(): Executes the processing pipeline.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config_params, logging_level=\"INFO\"): # Now we're passing in the confi_params dictionary already\n",
    "        \"\"\"Initializes the WeatherDataProcessor class instance.\n",
    "\n",
    "        Args:\n",
    "            config_params : A dictionary containing configuration parameters.\n",
    "            logging_level (str, optional): The logging level with a default of \"INFO\".\n",
    "\n",
    "        \"\"\"\n",
    "        self.weather_station_data = config_params['weather_csv_path']\n",
    "        self.patterns = config_params['regex_patterns']\n",
    "        self.weather_df = None  # Initialize weather_df as None or as an empty DataFrame\n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"Sets up logging for the WeatherDataProcessor class instance.\n",
    "\n",
    "        Args:\n",
    "            logging_level (str): The logging level.\n",
    "\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".WeatherDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        \"\"\"Maps weather station data to the DataFrame.\"\"\"\n",
    "        self.weather_df = read_from_web_CSV(self.weather_station_data)\n",
    "        self.logger.info(\"Successfully loaded weather station data from the web.\") \n",
    "        # Here, you can apply any initial transformations to self.weather_df if necessary.\n",
    "\n",
    "    \n",
    "    def extract_measurement(self, message):\n",
    "        \"\"\"Extracts a measurement from a message using regex patterns.\"\"\"\n",
    "        for key, pattern in self.patterns.items():\n",
    "            match = re.search(pattern, message)\n",
    "            if match:\n",
    "                self.logger.debug(f\"Measurement extracted: {key}\")\n",
    "                return key, float(next((x for x in match.groups() if x is not None)))\n",
    "        self.logger.debug(\"No measurement match found.\")\n",
    "        return None, None\n",
    "\n",
    "    def process_messages(self):\n",
    "        \"\"\"Processes messages to extract measurements.\"\"\"\n",
    "        if self.weather_df is not None:\n",
    "            result = self.weather_df['Message'].apply(self.extract_measurement)\n",
    "            self.weather_df['Measurement'], self.weather_df['Value'] = zip(*result)\n",
    "            self.logger.info(\"Messages processed and measurements extracted.\")\n",
    "        else:\n",
    "            self.logger.warning(\"weather_df is not initialized, skipping message processing.\")\n",
    "        return self.weather_df\n",
    "\n",
    "    def calculate_means(self):\n",
    "        \"\"\"Calculates the mean values of measurements.\"\"\"\n",
    "        if self.weather_df is not None:\n",
    "            means = self.weather_df.groupby(by=['Weather_station_ID', 'Measurement'])['Value'].mean()\n",
    "            self.logger.info(\"Mean values calculated.\")\n",
    "            return means.unstack()\n",
    "        else:\n",
    "            self.logger.warning(\"weather_df is not initialized, cannot calculate means.\")\n",
    "            return None\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Executes the processing pipeline.\"\"\"\n",
    "        self.weather_station_mapping()  # Load and assign data to weather_df\n",
    "        self.process_messages()  # Process messages to extract measurements\n",
    "        self.logger.info(\"Data processing completed.\")\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af300e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 19:55:23,562 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 19:55:23,578 - weather_data_processor.WeatherDataProcessor - INFO - Successfully loaded weather station data from the web.\n",
      "2024-02-26 19:55:23,649 - weather_data_processor.WeatherDataProcessor - INFO - Messages processed and measurements extracted.\n",
      "2024-02-26 19:55:23,649 - weather_data_processor.WeatherDataProcessor - INFO - Data processing completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Temperature', 'Pollution_level', 'Rainfall'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from field_data_processor import FieldDataProcessor\n",
    "from weather_data_processor import WeatherDataProcessor\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params = { \n",
    "    \"sql_query\": \"\"\"\n",
    "                SELECT *\n",
    "                FROM geographic_features\n",
    "                LEFT JOIN weather_features USING (Field_ID)\n",
    "                LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "                LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\", # Insert your SQL query\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db', # Insert the db_path of the database\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}, # Insert the disctionary of columns we want to swop the names of,\n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}, # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "    # Paste in your previous dictionary data in here\n",
    "\n",
    "    # Add two new keys\n",
    "   \"weather_csv_path\":  \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the URL for the weather station data\n",
    "    \"regex_patterns\" : {\n",
    "    'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "     'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "    } , # Insert the regex pattern we used to process the messages\n",
    "} # Paste in your config_params dictionary here\n",
    "\n",
    "# Ignoring the field data for now.\n",
    "# field_processor = FieldDataProcessor(config_params)\n",
    "# field_processor.process()\n",
    "# field_df = field_processor.df\n",
    "\n",
    "weather_processor = WeatherDataProcessor(config_params)\n",
    "weather_processor.process()\n",
    "weather_df = weather_processor.weather_df\n",
    "\n",
    "weather_df['Measurement'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d54794",
   "metadata": {},
   "source": [
    "### Validating our data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0350663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 19:38:35,671 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 19:38:35,933 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 19:38:35,934 - field_data_processor.FieldDataProcessor - INFO - Successfully loaded data.\n",
      "2024-02-26 19:38:35,935 - field_data_processor.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-02-26 19:38:36,993 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 19:38:38,095 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 19:38:38,095 - weather_data_processor.WeatherDataProcessor - INFO - Successfully loaded weather station data from the web.\n",
      "2024-02-26 19:38:38,154 - weather_data_processor.WeatherDataProcessor - INFO - Messages processed and measurements extracted.\n",
      "2024-02-26 19:38:38,156 - weather_data_processor.WeatherDataProcessor - INFO - Data processing completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from field_data_processor import FieldDataProcessor\n",
    "from weather_data_processor import WeatherDataProcessor\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params = { \n",
    "    \"sql_query\": \"\"\"\n",
    "                SELECT *\n",
    "                FROM geographic_features\n",
    "                LEFT JOIN weather_features USING (Field_ID)\n",
    "                LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "                LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\", # Insert your SQL query\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db', # Insert the db_path of the database\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}, # Insert the disctionary of columns we want to swop the names of,\n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}, # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "    # Paste in your previous dictionary data in here\n",
    "\n",
    "    # Add two new keys\n",
    "   \"weather_csv_path\":  \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the URL for the weather station data\n",
    "    \"regex_patterns\" : {\n",
    "    'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "     'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "    } , # Insert the regex pattern we used to process the messages\n",
    "} # Paste in your config_params dictionary here\n",
    "\n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "weather_processor = WeatherDataProcessor(config_params)\n",
    "weather_processor.process()\n",
    "weather_df = weather_processor.weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5456e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.7, pytest-8.0.2, pluggy-1.4.0 -- C:\\Users\\user\\anaconda3\\envs\\Alx_python\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: C:\\Users\\user\\AAApython ALX\\tests\\test\\validating data\\final\n",
      "plugins: anyio-4.2.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 7 items\n",
      "\n",
      "validate_data.py::test_read_weather_DataFrame_shape \u001b[32mPASSED\u001b[0m\u001b[32m               [ 14%]\u001b[0m\n",
      "validate_data.py::test_read_field_DataFrame_shape \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 28%]\u001b[0m\n",
      "validate_data.py::test_weather_DataFrame_columns \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 42%]\u001b[0m\n",
      "validate_data.py::test_field_DataFrame_columns \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 57%]\u001b[0m\n",
      "validate_data.py::test_field_DataFrame_non_negative_elevation \u001b[32mPASSED\u001b[0m\u001b[32m     [ 71%]\u001b[0m\n",
      "validate_data.py::test_crop_types_are_valid \u001b[32mPASSED\u001b[0m\u001b[32m                       [ 85%]\u001b[0m\n",
      "validate_data.py::test_positive_rainfall_values \u001b[32mPASSED\u001b[0m\u001b[32m                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 1.82s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "Deleted sampled_weather_df.csv\n",
      "Deleted sampled_field_df.csv\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytest\n",
    "\n",
    "weather_df.to_csv('sampled_weather_df.csv', index=False)\n",
    "field_df.to_csv('sampled_field_df.csv', index=False)\n",
    "\n",
    "!pytest validate_data.py -v\n",
    "\n",
    "import os# Define the file paths\n",
    "weather_csv_path = 'sampled_weather_df.csv'\n",
    "field_csv_path = 'sampled_field_df.csv'\n",
    "\n",
    "# Delete sampled_weather_df.csv if it exists\n",
    "if os.path.exists(weather_csv_path):\n",
    "    os.remove(weather_csv_path)\n",
    "    print(f\"Deleted {weather_csv_path}\")\n",
    "else:\n",
    "    print(f\"{weather_csv_path} does not exist.\")\n",
    "\n",
    "# Delete sampled_field_df.csv if it exists\n",
    "if os.path.exists(field_csv_path):\n",
    "    os.remove(field_csv_path)\n",
    "    print(f\"Deleted {field_csv_path}\")\n",
    "else:\n",
    "    print(f\"{field_csv_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca09816",
   "metadata": {},
   "source": [
    "# Validating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c5ed490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 19:56:34,212 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 19:56:34,408 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 19:56:34,408 - field_data_processor.FieldDataProcessor - INFO - Successfully loaded data.\n",
      "2024-02-26 19:56:34,408 - field_data_processor.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-02-26 19:56:35,408 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 19:56:36,418 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 19:56:36,418 - weather_data_processor.WeatherDataProcessor - INFO - Successfully loaded weather station data from the web.\n",
      "2024-02-26 19:56:36,501 - weather_data_processor.WeatherDataProcessor - INFO - Messages processed and measurements extracted.\n",
      "2024-02-26 19:56:36,501 - weather_data_processor.WeatherDataProcessor - INFO - Data processing completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from field_data_processor import FieldDataProcessor\n",
    "from weather_data_processor import WeatherDataProcessor\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params =  { \n",
    "    \"sql_query\": \"\"\"\n",
    "                SELECT *\n",
    "                FROM geographic_features\n",
    "                LEFT JOIN weather_features USING (Field_ID)\n",
    "                LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "                LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\", # Insert your SQL query\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db', # Insert the db_path of the database\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}, # Insert the disctionary of columns we want to swop the names of,\n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}, # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "    # Paste in your previous dictionary data in here\n",
    "\n",
    "    # Add two new keys\n",
    "   \"weather_csv_path\":  \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the URL for the weather station data\n",
    "    \"regex_patterns\" : {\n",
    "    'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "     'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "    } , # Insert the regex pattern we used to process the messages\n",
    "} # Paste in your config_params dictionary here   # Paste in your previous dictionary data in here\n",
    "\n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "weather_processor = WeatherDataProcessor(config_params)\n",
    "weather_processor.process()\n",
    "weather_df = weather_processor.weather_df\n",
    "\n",
    "# Rename 'Ave_temps' in field_df to 'Temperature' to match weather_df\n",
    "field_df.rename(columns={'Ave_temps': 'Temperature'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbcc03",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5833b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "\n",
    "# Now, the measurements_to_compare can directly use 'Temperature', 'Rainfall', and 'Pollution_level'\n",
    "measurements_to_compare = ['Temperature', 'Rainfall', 'Pollution_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2b52be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def filter_field_data(df, station_id, measurement):\n",
    "    \n",
    "    \"\"\"\n",
    "    Filter the DataFrame to include rows where Weather_station  = to station_id,\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The dataframe.\n",
    "    - station_id :  weather station ID.\n",
    "    - measurement (str): T measurement column to be returned.\n",
    "\n",
    "    Returns:\n",
    "    - Series: A pandas Series containing the specified measurement for the selected weather station.\n",
    "    \"\"\"\n",
    "    \n",
    "    return df[df['Weather_station'] == station_id][measurement]\n",
    "\n",
    "    return df\n",
    "    \n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "123ae3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       13.35\n",
       "2       13.30\n",
       "8       12.80\n",
       "10      13.70\n",
       "14      13.35\n",
       "        ...  \n",
       "5627    13.30\n",
       "5630    14.25\n",
       "5632    11.00\n",
       "5638    13.30\n",
       "5642    12.85\n",
       "Name: Temperature, Length: 1375, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "field_values = filter_field_data(field_df, station_id, measurement)\n",
    "field_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99f9178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1375,), First value: 13.35 \n"
     ]
    }
   ],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "field_values = filter_field_data(field_df, station_id, measurement)\n",
    "print(f\"Shape: {field_values.shape}, First value: {field_values.iloc[0]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50110261",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "\n",
    "def filter_weather_data(df, station_id, measurement):\n",
    "    \"\"\"\n",
    "    Filter weather data DataFrame. \n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame\n",
    "    - station_id : The ID of the weather station ID.\n",
    "    - measurement : The name of the measurement .\n",
    "\n",
    "    Returns:\n",
    "    - Series: A pandas Series \n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_data = df[(df['Weather_station_ID'] == station_id) & (df['Measurement'] == measurement)]['Value']\n",
    "    if filtered_data.empty:\n",
    "        print(f\"Warning: No data found for station ID {station_id} and measurement '{measurement}'.\")\n",
    "        return pd.Series()\n",
    "    return filtered_data\n",
    "\n",
    "    return df\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d919e41",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "**Input 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a948c941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       12.82\n",
       "2       14.53\n",
       "29      14.28\n",
       "32      12.87\n",
       "67      13.13\n",
       "        ...  \n",
       "1804    12.77\n",
       "1805    14.13\n",
       "1817    13.14\n",
       "1833    14.14\n",
       "1834    13.61\n",
       "Name: Value, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "\n",
    "weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "weather_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "581e8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (100,), First value: 12.82\n"
     ]
    }
   ],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "\n",
    "weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "\n",
    "print(f\"Shape: {weather_values.shape}, First value: {weather_values.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e45878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def run_ttest(Column_A, Column_B):\n",
    "    \"\"\"\n",
    "    Run a  t-test on two columns of data.\n",
    "\n",
    "    Parameters:\n",
    "    - Column_A : The first column with the data.\n",
    "    - Column_B : The second column with the data.\n",
    "\n",
    "    Returns:\n",
    "    a tuple\n",
    "    \"\"\"    \n",
    "    t_statistic, p_value = ttest_ind(Column_A, Column_B, equal_var=False)  # Welch's t-test\n",
    "\n",
    "\n",
    "    return t_statistic, p_value\n",
    "    \n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3b07adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-stat: -0.11632, p-value: 0.90761\n"
     ]
    }
   ],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "field_values = filter_field_data(field_df, station_id, measurement)\n",
    "weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_val = run_ttest(field_values, weather_values)\n",
    "print(f\"T-stat: {t_stat:.5f}, p-value: {p_val:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c884eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "\n",
    "def print_ttest_results(station_id, measurement, p_val, alpha):\n",
    "    \"\"\"\n",
    "    Interprets and prints the results of a t-test based on the p-value.\n",
    "    \"\"\"\n",
    "    if p_val < alpha:\n",
    "        print(f\"   Significant difference in {measurement} detected at Station  {station_id}, (P-Value: {p_val:.5f} < {alpha}). Null hypothesis rejected.\")\n",
    "    else:\n",
    "        print(f\"   No significant difference in {measurement} detected at Station  {station_id}, (P-Value: {p_val:.5f} > {alpha}). Null hypothesis not rejected.\")\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59929bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   No significant difference in Temperature detected at Station  0, (P-Value: 0.90761 > 0.05). Null hypothesis not rejected.\n"
     ]
    }
   ],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "field_values = filter_field_data(field_df, station_id, measurement)\n",
    "weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_val = run_ttest(field_values, weather_values)\n",
    "print_ttest_results(station_id, measurement, p_val, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89bf6f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def hypothesis_results(field_df, weather_df, list_measurements_to_compare, alpha = 0.05):\n",
    "    \n",
    "    \"\"\"\n",
    "    Interprets and prints the results of the t-test.\n",
    "\n",
    "    Parameters:\n",
    "    - station_id :  the weather station ID.\n",
    "    - measurement : The measurement of comparison.\n",
    "    - p_val : The p-value from the t-test.\n",
    "    - alpha : significance level.\n",
    "\n",
    "    Returns:\n",
    "     Prints the interpretation of the t-test result.\n",
    "    \"\"\"\n",
    "    station_ids = sorted(field_df['Weather_station'].unique())\n",
    "    for station_id in station_ids :\n",
    "      for measurement in list_measurements_to_compare:\n",
    "            field_values = filter_field_data(field_df, station_id, measurement)\n",
    "            weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "            t_stat, p_val = run_ttest(field_values, weather_values)\n",
    "            print_ttest_results(station_id, measurement, p_val, alpha)\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea120b",
   "metadata": {},
   "source": [
    "**Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1fafd76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   No significant difference in Temperature detected at Station  0, (P-Value: 0.90761 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Rainfall detected at Station  0, (P-Value: 0.21621 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Pollution_level detected at Station  0, (P-Value: 0.56418 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Temperature detected at Station  1, (P-Value: 0.47241 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Rainfall detected at Station  1, (P-Value: 0.54499 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Pollution_level detected at Station  1, (P-Value: 0.24410 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Temperature detected at Station  2, (P-Value: 0.88671 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Rainfall detected at Station  2, (P-Value: 0.36466 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Pollution_level detected at Station  2, (P-Value: 0.99388 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Temperature detected at Station  3, (P-Value: 0.66445 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Rainfall detected at Station  3, (P-Value: 0.39847 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Pollution_level detected at Station  3, (P-Value: 0.15466 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Temperature detected at Station  4, (P-Value: 0.88575 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Rainfall detected at Station  4, (P-Value: 0.33237 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Pollution_level detected at Station  4, (P-Value: 0.21508 > 0.05). Null hypothesis not rejected.\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "hypothesis_results(field_df, weather_df, measurements_to_compare, alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
